{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src='img/ms_logo.jpeg' height=40% width=40%></center>\n",
    "\n",
    "<center><h1>Support Vector Machines</h1></center>\n",
    "\n",
    "In this notebook, we'll cover one of the major algorithms used in Supervised Learning--**_Support Vector Machines_** (or _SVMs_ for short!). We'll start by playing around with a visual implementation to gain an intuition for how SVMs work, and then we'll grab an SVM implementation from `sklearn` and use to it make some classifcations on a real world data set.\n",
    "\n",
    "<center><h3>How Support Vector Machines Work</h3></center>\n",
    "\n",
    "at first glance, SVMs are similar to other supervised learning algorithms such as Logistic Regression, because the algorithm find the optimal line for a decision boundary.  However, unlike Logistic Regression, SVMs don't just find a line for the decision boundary--they try to maximize the margin between the two sides. \n",
    "\n",
    "<center><img src='img/svm_boundary.png' height=40% width=40%></center>\n",
    "\n",
    "The points that touch the sides of the margin are called **_support vectors_**.  By maximizing the margin by finding support vectors, this has the effect of \"balancing\" the the decision boundary so that it evenly splits the area between the two classes.  This is not always the case with Logistic Regression--see the image below for a visual example.  \n",
    "\n",
    "<center><img src=\"img/svm_vs_lr.png\"></center>\n",
    "\n",
    "Notice that on the image on the right, the line is a bit skewed through the datapoints.  This is a problem that can occur with Logistic Regression, since it's job is to fit a line that linearly separates the two classes. The line in the image on the right _technically_ accomplishes this task, but we can see by looking at the decision boundary that this is not optimal. Contrast this with the decision boundary on the left, which splits the area between the two classes perfectly.  \n",
    "\n",
    "<center><h3>Linear Separability and Kernel Methods</h3></center>\n",
    "\n",
    "SVMs are not perfect, however--they only work when the data is **_linearly separable_**--that is, the decision boundary is linear, and can be drawn as a straight line.  Take a look at the picture below, and consider where you would draw the ideal decision boundary to split the two--remember, it has to be a straight line!\n",
    "\n",
    "<center><img src='img/before_kernel.png' height=50% width=50%></center>\n",
    "\n",
    "The data is non linearly separable, so we can't draw a decision boundary--or can we?  This is where the cool part of SVMs comes in--what if we mapped the data to a **_higher-dimension space_**--maybe we could draw a decision boundary there?\n",
    "\n",
    "<center><img src='img/after_kernel.png'></center>\n",
    "\n",
    "Ah, there it is! In this higher dimensional space, we can see an easy place to draw a linear decision boundary.  It's important to note that in 2 dimensions, our decision boundary looks like a straight line--but for this data, in its current form, our decision boundary will need to look like a piece of paper (with no thickness). This is because our decision boundary will always have one less dimension than the data we are trying to find a decision boundary for.  If our data has 4 dimensions (which we can't visualize), then our decision boundary would be a **_hyperplane_** that would look like a rectangle.  We can generalize this rule to say that for any dataset with \\[n\\] dimensions, our decision boundary will have \\[n - 1\\] dimensions. \n",
    "\n",
    "The process of mapping data to a higher-dimensional space is called the **_Kernel Method_**.  There are several different kernels that are typically used, but the most common ones you'll typically need to know are the **_Polynomial Kernel_** and the **_Radial Basis Function (RBF)_**--these are complicated data transformations that any ML library worth its salt can handle for you. You don't need to know the math behind them, but you should definitely be aware that they exist, and that they are tools in your ML toolbox for SVMs!\n",
    "\n",
    "<center>The Final Step</center>\n",
    "\n",
    "Let's review what we've done so far:\n",
    "\n",
    "1.  Determined that the data is not linearly separable in its current form.\n",
    "2.  Mapped the data to a higher dimensional space using a kernel method.\n",
    "3.  Found a linear decision boundary in the higher dimensional space. \n",
    "\n",
    "Now what?\n",
    "\n",
    "Now that we've identified support vectors that allow us to linearly separate the data in a higher dimensional space, all that we need to do is to bring the data (and the decision boundary) back to our original, lower-dimensional space.  If we visualize the decision boundary for our data in the lower-dimensional space, it will appear as a circle:\n",
    "\n",
    "<center><img src='img/kernel_with_boundary.png'></center>\n",
    "\n",
    "It's important to understand that although our decision boundary isn't linear in this lower-dimensional space, that's okay--we found a linear decision boundary in a higher-dimensional space and made our classifications, so we didn't actually break the rules of Support Vector Machines.  \n",
    "\n",
    "<center><h3>Playing Around with SVMs</h3></center>\n",
    "\n",
    "To make learning how SVMs work a bit easier, the `sklearn` community has built an awesome interactive visualization that lets users plot points and fit an SVM for binary classification. We **_highly recommend_** running this python script and getting a feel for how SVMs work--plot different data points and see how the decision boundary changes, try different kernel methods, visualize the decision surface of the SVM, etc.  You'll find all of these activities very useful, and very interesting.  \n",
    "\n",
    "Check out [this link](http://scikit-learn.org/stable/auto_examples/applications/svm_gui.html#sphx-glr-auto-examples-applications-svm-gui-py) to see the page on sklearn.org that gives an example of how everything works. **_To download the file, download and run the python script linked at the bottom of the page (use the script version, not the jupyter notebook!)_**\n",
    "\n",
    "\n",
    "<center><h2>Challenge: Classifications with SVMs</h2></center>\n",
    "\n",
    "For the remainder of this notebook, you'll use everything you've learned in DS2 to use a Support Vector Classifier on the [Wisconsin Breast Cancer Dataset](http://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+%28Diagnostic%29).  Note that you **_do not need to download the dataset_**, as it comes preloaded as a sample in sklearn.  To get the data, just use the `load_breast_cancer()` method found within `sklearn.datasets`.\n",
    "\n",
    "**_Challenge_**:\n",
    "\n",
    "1.  Import and explore the dataset.  Recall the `load_breast_cancer()` method will return an object that contains the data in `.data`, the labels in `.target`, and the column names in `.feature_names` attributes.  \n",
    "2.  Build a **_Correlation Heatmap_** using **_Seaborn_** to check for each feature's correlation with the labels. \n",
    "3.  Build a second **_Correlation Heatmap_** using **_Seaborn_** to check for **_mutlicollinearity_** between features.  \n",
    "4.  Scale and transform the data using a `StandardScaler()` object and any appropriate methods it contains.\n",
    "5.  Split the newly scaled data into training and testing sets using `train_test_split()`.\n",
    "6.  Create an `SVC()` object, which can be found in `sklearn.svm`\n",
    "7.  Fit the model to the scaled data. \n",
    "8.  Use your validation data to check the accuracy metrics for your model.  \n",
    "\n",
    "\n",
    "**_Stretch Challenge_**: \n",
    "\n",
    "1. Try different parameters such as different kernels to see how it affects the overall performance of the model.  For a full list of the tunable parameters you can use with an SVC, see [the documentation](http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html) on sklearn.org. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code below!\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
